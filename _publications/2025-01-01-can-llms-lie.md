---
title: "Can LLMs Lie? Investigation beyond Hallucination"
collection: publications
category: manuscripts
permalink: /publication/2025-01-01-can-llms-lie
excerpt: 'We explore the bottom-up formation and top-down control of deceptive responses in LLMs, analyzing how deception emerges and propagates to the next prediction.'
date: 2025-01-01
venue: 'arXiv preprint'
paperurl: 'https://arxiv.org/abs/2501.12346'
citation: 'Huan, H., Prabhudesai, M., Wu, M., Jaiswal, S., & Pathak, D. (2025). &quot;Can LLMs Lie? Investigation beyond Hallucination.&quot; <i>arXiv preprint</i>.'
---

This work explores the bottom-up formation and top-down control of deceptive responses in Large Language Models (LLMs), analyzing how deception emerges and propagates to the next prediction. We conduct layer-to-neuron level experiments to analyze how lying intent and incorrect facts are encoded in MLP and propagate through Attention.

[Download paper here](https://arxiv.org/abs/2501.12346)

Recommended citation: Huan, H., Prabhudesai, M., Wu, M., Jaiswal, S., & Pathak, D. (2025). "Can LLMs Lie? Investigation beyond Hallucination." <i>arXiv preprint</i>.

